{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJtbfqwhW8rMapPaIfjyIk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VinayNagamallaD9/SmartApplicationPerformanceMonitoring-Auto-Scaling/blob/main/Jupyter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement :**\n",
        "\n",
        "Develop a machine learning model for Smart Application Performance Monitoring and Auto-Scaling based on the provided problem description, skills demonstrated, and desired deliverable. The model should predict demand, detect anomalies, and automatically scale resources."
      ],
      "metadata": {
        "id": "u7uqDjjMw9Fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data collection and preprocessing**\n",
        "* As  we  don’t have real data, so let's make some fake data that looks like app performance.\n",
        "\n",
        "* Let us also include some missing parts and weird values to make it more realistic.\n",
        "\n",
        "* At last we will clean the data to show how it is done in real situations.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0W-oIUuSxNQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Creating a dataset\n",
        "# Assuming hourly data over a year\n",
        "date_rng = pd.date_range(start='2024-01-01', end='2024-12-31 23:00:00', freq='H')\n",
        "df = pd.DataFrame(date_rng, columns=['timestamp'])\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "df = df.set_index('timestamp')\n",
        "\n",
        "# Generate synthetic performance metrics and demand\n",
        "np.random.seed(15) # for reproducibility\n",
        "df['cpu_usage'] = np.random.rand(len(df)) * 80 + 10         # Percentage(%)\n",
        "df['memory_usage'] = np.random.rand(len(df)) * 70 + 15      # Percentage(%)\n",
        "df['network_traffic'] = np.random.rand(len(df)) * 500 + 100 # Mbps\n",
        "df['response_time'] = np.random.rand(len(df)) * 200 + 50    # Milliseconds(MS)\n",
        "df['demand'] = np.random.rand(len(df)) * 1000 + 200         # Requests/hour\n",
        "\n",
        "# Adding  some seasonality and trend to demand\n",
        "df['demand'] = df['demand'] + 200 * np.sin(np.arange(len(df)) / (24 * 30) * 2 * np.pi) # Monthly seasonality\n",
        "df['demand'] = df['demand'] + np.arange(len(df)) * 0.05 # Linear trend\n",
        "\n",
        "# Including missing values\n",
        "for col in ['cpu_usage', 'memory_usage', 'network_traffic', 'response_time', 'demand']:\n",
        "    df.loc[df.sample(frac=0.05).index, col] = np.nan\n",
        "\n",
        "# Introduce outliers\n",
        "for col in ['cpu_usage', 'memory_usage', 'network_traffic', 'response_time', 'demand']:\n",
        "    outlier_indices = df.sample(frac=0.01).index\n",
        "    df.loc[outlier_indices, col] = df.loc[outlier_indices, col] * np.random.choice([2, 0.5]) + np.random.rand() * 500\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6mD3OVIyxVOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Collection in Real Scenario** :\n",
        "\n",
        "In real life, data is collected from tools like Prometheus, Datadog, or application logs. These tools track things like CPU, memory, network, and response time. The data is gathered at regular time intervals and stored in databases for analysis."
      ],
      "metadata": {
        "id": "Ptj9LyaNxYZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing :**\n",
        "\n",
        " we deal with :\n",
        " * Fix missing values using fill methods or by removing them.\n",
        "\n",
        "* Handle outliers using stats like IQR or Z-score.\n",
        "\n",
        "* Format the data with time as the index, and make sure it has a consistent time gap between entries.\n",
        "\n"
      ],
      "metadata": {
        "id": "ibVO8oIOxanK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "# Handle missing values by interpolation\n",
        "df_preprocessed = df.copy()\n",
        "df_preprocessed = df_preprocessed.interpolate(method='time')\n",
        "\n",
        "# Detecting outliers by InterQuartileRange\n",
        "for col in ['cpu_usage', 'memory_usage', 'network_traffic', 'response_time', 'demand']:\n",
        "    Q1 = df_preprocessed[col].quantile(0.25)\n",
        "    Q3 = df_preprocessed[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    # Cap outliers\n",
        "    df_preprocessed[col] = np.where(df_preprocessed[col] < lower_bound, lower_bound, df_preprocessed[col])\n",
        "    df_preprocessed[col] = np.where(df_preprocessed[col] > upper_bound, upper_bound, df_preprocessed[col])\n",
        "\n",
        "\n",
        "# Displaying Preprocessed Data\n",
        "display(df_preprocessed.head())\n",
        "display(df_preprocessed.info())\n"
      ],
      "metadata": {
        "id": "CS_BqyDkxYHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time series forecasting\n",
        "\n",
        "\n",
        "Now Implementing a time series forecasting model to predict future resource demand based on historical patterns and seasonality.\n",
        "\n",
        "\n",
        "We use **Prophet** tool which helps  to predict future data trends based on past time-series data."
      ],
      "metadata": {
        "id": "FPwWPzM7xen-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prepare data for Prophet\n",
        "# Prophet needs two columns: 'ds' for date/time & 'y' for value you want to predict.\n",
        "df_prophet = df_preprocessed.reset_index()[['timestamp', 'demand']].rename(columns={'timestamp': 'ds', 'demand': 'y'})\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "train_size = int(len(df_prophet) * 0.8)\n",
        "train_df = df_prophet[:train_size]\n",
        "test_df = df_prophet[train_size:]\n",
        "\n",
        "# Initialize & train the Prophet model\n",
        "model = Prophet(seasonality_mode='additive', daily_seasonality=True)\n",
        "model.fit(train_df)\n",
        "\n",
        "# Make predictions on test set\n",
        "future = model.make_future_dataframe(periods=len(test_df), freq='H', include_history=False)\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# Evaluate the model\n",
        "# Merge actual values with predictions for evaluation\n",
        "evaluation_df = test_df.set_index('ds').join(forecast.set_index('ds')[['yhat']])\n",
        "evaluation_df.dropna(inplace=True)\n",
        "# Using dtopna to drop nullvalues\n",
        "\n",
        "mae = mean_absolute_error(evaluation_df['y'], evaluation_df['yhat'])\n",
        "mse = mean_squared_error(evaluation_df['y'], evaluation_df['yhat'])\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f'Mean Absolute Error: {mae}')\n",
        "print(f'Mean Square Error: {mse}')\n",
        "print(f'Root Mean Square Error: {rmse}')\n",
        "\n",
        "# Plotting components of forecast\n",
        "fig1 = model.plot_components(forecast)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PhusoFwaxgw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Performance anomaly detection**\n",
        "\n",
        "We need to develop a module to detect performance anomalies (e.g., sudden spikes in response time, errors) that might indicate a need for immediate scaling or investigation.\n"
      ],
      "metadata": {
        "id": "DY3SUuwUxjhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate rolling average and standard deviation (for 24 hours)\n",
        "window_size = 24\n",
        "df_preprocessed['response_time_mean'] = df_preprocessed['response_time'].rolling(window=window_size).mean()\n",
        "df_preprocessed['response_time_std'] = df_preprocessed['response_time'].rolling(window=window_size).std()\n",
        "\n",
        "# Set upper and lower limits by standard deviations\n",
        "n_std = 3\n",
        "df_preprocessed['response_time_upper_bound'] = df_preprocessed['response_time_mean'] + n_std * df_preprocessed['response_time_std']\n",
        "df_preprocessed['response_time_lower_bound'] = df_preprocessed['response_time_mean'] - n_std * df_preprocessed['response_time_std']\n",
        "\n",
        "# Mark points outside these limits as anomalies\n",
        "df_preprocessed['response_time_anomaly'] = 0\n",
        "df_preprocessed.loc[df_preprocessed['response_time'] > df_preprocessed['response_time_upper_bound'], 'response_time_anomaly'] = 1\n",
        "df_preprocessed.loc[df_preprocessed['response_time'] < df_preprocessed['response_time_lower_bound'], 'response_time_anomaly'] = 1\n",
        "\n",
        "# Displaying first 10 rows with all related columns\n",
        "display(df_preprocessed[['response_time', 'response_time_mean', 'response_time_std',\n",
        "                         'response_time_upper_bound', 'response_time_lower_bound', 'response_time_anomaly']].head(10))\n",
        "\n",
        "# Displaying Anomaly rows only\n",
        "display(df_preprocessed[df_preprocessed['response_time_anomaly'] == 1].head())\n"
      ],
      "metadata": {
        "id": "KlYQxFwIxpJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Scaling decision engine**\n",
        "\n",
        "\n",
        "Define the scaling decision engine function that takes predicted demand, current resource utilization, and anomaly flags as input and outputs scaling actions based on predefined policies. Then apply this engine to the evaluation data and store the results.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2kxWYkqNxs64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to decide scaling based on demand and anomalies\n",
        "def scaling_decision_engine(predicted_demand, response_time_anomaly, up=1200, down=800):\n",
        "    if response_time_anomaly == 1:\n",
        "        return 'scale_up'\n",
        "    elif predicted_demand > up:\n",
        "        return 'scale_up'\n",
        "    elif predicted_demand < down:\n",
        "        return 'scale_down'\n",
        "    else:\n",
        "        return 'no_action'\n",
        "\n",
        "# Combine forecast with needed metrics\n",
        "evaluation_data = evaluation_df.copy()\n",
        "evaluation_data = evaluation_data.join(df_preprocessed[['response_time_anomaly']])\n",
        "\n",
        "# Apply the scaling decision for each row\n",
        "evaluation_data['scaling_action'] = evaluation_data.apply(\n",
        "    lambda row: scaling_decision_engine(row['yhat'], row['response_time_anomaly']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Show the results\n",
        "display(evaluation_data.head())\n",
        "display(evaluation_data['scaling_action'].value_counts())\n"
      ],
      "metadata": {
        "id": "RVCE1DOYxv6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Scaling execution module**\n",
        "Our task is to :\n",
        "\n",
        "* Implement a module to execute the scaling decisions by interacting with the cloud provider's API or infrastructure management tools to adjust resources\n"
      ],
      "metadata": {
        "id": "NLvJ5U3Zx1J5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to simulate scaling actions\n",
        "def execute_scaling_action(action):\n",
        "    if action == 'scale_up':\n",
        "        print(\"Scaling up: Adding more resources...\")\n",
        "    elif action == 'scale_down':\n",
        "        print(\"Scaling down: Reducing resources...\")\n",
        "    elif action == 'no_action':\n",
        "        pass\n",
        "    else:\n",
        "        print(f\"Unknown action: {action}\")\n",
        "\n",
        "# Run the simulation\n",
        "print(\"Running scaling simulation:\")\n",
        "evaluation_data['scaling_action'].apply(execute_scaling_action)\n"
      ],
      "metadata": {
        "id": "Ey3m9f4bx50v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model evaluation and refinement**\n",
        "\n",
        "Evaluate the performance of the forecasting and anomaly detection models. Monitor the effectiveness of the auto-scaling system in maintaining desired performance levels and resource utilization. Refine the models and scaling policies based on the evaluation results.\n"
      ],
      "metadata": {
        "id": "d7nL0Thwx9R1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Calculate and print evaluation metrics for the Prophet forecasting model\n",
        "mae = mean_absolute_error(evaluation_df['y'], evaluation_df['yhat'])\n",
        "mse = mean_squared_error(evaluation_df['y'], evaluation_df['yhat'])\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(\"Forecasting Model Evaluation Metrics:\")\n",
        "print(f'Mean Absolute Error: {mae}')\n",
        "print(f'Mean Square Error: {mse}')\n",
        "print(f'Root Mean Square Error : {rmse}')\n",
        "\n",
        "# Anomaly detection evaluation\n",
        "anomaly_counts = evaluation_data['response_time_anomaly'].value_counts()\n",
        "print(\"\\nAnomaly Detection Evaluation:\")\n",
        "print(f\"Number of detected anomalies: {anomaly_counts.get(1, 0)}\")\n",
        "print(\"Distribution of anomaly flags:\")\n",
        "print(anomaly_counts)\n",
        "\n",
        "# Analyze the distribution of scaling actions\n",
        "scaling_action_counts = evaluation_data['scaling_action'].value_counts()\n",
        "print(\"\\nScaling Action Distribution:\")\n",
        "print(scaling_action_counts)\n",
        "\n"
      ],
      "metadata": {
        "id": "0VJ700OGyAXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to track if auto-scaling is working well**\n",
        "\n",
        "How to Know if Auto-scaling Is Working ?\n",
        "\n",
        "Look at these key things :\n",
        "\n",
        "\n",
        "1. **Resource Usage:** Check CPU, memory, and network to make sure you're not wasting or overloading.\n",
        "\n",
        "2. **App Speed:** Track how fast the app responds, especially during busy times.\n",
        "3. **Cost:** Keep an eye on how much you're spending over time.\"\n",
        "4. **Scaling Frequency:** See how often the system scales up/down and how fast it reacts.\n",
        "5. Use **tools** like Prometheus, Grafana, or cloud monitoring services to collect and visualize this info.\"\n",
        "6. **Review** **& adjust** scaling settings weekly or monthly based on system performance\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0RTtO4J7yDAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**:\n",
        "\n",
        "\n",
        "*   We made fake data to act like an app's performance — like CPU use, memory, and user traffic.\n",
        "*  We cleaned the data and fixed missing or unusual values.\n",
        "*   We used a model (Prophet) to guess future demand. It did a good job.\n",
        "*   We checked for weird spikes in response time and found 105 such cases.\n",
        "*   A rule-based system decided when to scale up, scale down, or do nothing. Most of the time, it said \"no action,\" but it scaled up 105 times.\n",
        "\n",
        "*   We also made a fake version of what scaling would look like in real life.\n",
        "*   Each part of the system was built separately, making it easy to understand and change later.\n",
        "\n",
        "* To predict future demand, we used a forecasting model called Prophet. It gave decent results with:\n",
        "\n",
        "\n",
        "\n",
        "1.   Mean Absolute Error : 67.44\n",
        "2.   Mean Square Error: 6613.51\n",
        "3.   Root Mean Square Error : 81.32\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fh8GPh2OyHjr"
      }
    }
  ]
}